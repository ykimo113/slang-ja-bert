# Sentiment Analysis Model considering Internet Slang

## 発表実績
第16回 データ工学と情報マネジメントに関するフォーラム([ポスター資料](https://drive.google.com/file/d/1epG7FTqm2PCWhgCPYqLY50oq5AhdWX2C/view?usp=drive_link))

## 実行環境の制限
* `transformers >= 4.34.1`
* `accelerate >= 0.24.0`
* 学習に必要なため、このバージョンは厳守してください。

## 実行方法
* アクセラレータとしてGPUを選択してください。
* 上のコードから順に実行してください。

## 提案手法
<img width="790" alt="modelpic" src="https://github.com/ykimo113/slangBert/assets/153272953/d4bf3bff-ef0c-43ef-af83-a63dd045edf3">

1. 既存BERTモデルでインターネットスラングを含むテキストをトークン化  
2. トークン化の結果から、誤識別された単語を抽出  
3. 誤識別された単語+インターネット上から収集した単語から、インターネットスラング辞書を作成  
4. トークナイザーの辞書に、インターネットスラング辞書をユーザー辞書として追加  
5. インターネットスラング辞書に基づいて、インターネット上からスラングを含むテキストを収集  
6. 収集したテキストから、追加学習用のインターネットスラングコーパスを作成  
7. インターネットスラングコーパスを用いて、辞書を追加したモデルを追加学習  
8. 追加学習を行ったモデルに対して、感情ラベル付きデータセットを用いてファインチューニング  
9. 感情分析タスクの実行

### 辞書の追加(3.および4.)
<img width="1034" alt="辞書追加 概要" src="https://github.com/ykimo113/slangBert/assets/153272953/59c63175-ef05-402b-8d34-9fb33bfe5090">

* 使用するシステム辞書:`mecab-ipadic-NEologd`
* 誤分割された語句をユーザ辞書としてトークナイザに追加
* 動詞等は活用形も含めて追加する

### 追加学習(8.)

* 使用モデル:`bert-base-japanese-whole-word-masking`(東北大BERT)
* 辞書に基づいたコーパスでMLMタスクに対して追加学習を行う
* 追加した語句に対して適切なベクトルを獲得する



## 実験結果
### 追加学習の学習曲線

### 追加学習後の類義語

### 追加学習後の感情分析タスク

